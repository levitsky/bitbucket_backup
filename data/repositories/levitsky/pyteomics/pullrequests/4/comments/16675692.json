{"links": {"self": {"href": "data/repositories/levitsky/pyteomics/pullrequests/4/comments/16675692.json"}, "html": {"href": "#!/levitsky/pyteomics/pull-requests/4/_/diff#comment-16675692"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 4, "links": {"self": {"href": "data/repositories/levitsky/pyteomics/pullrequests/4.json"}, "html": {"href": "#!/levitsky/pyteomics/pull-requests/4"}}, "title": "Added an IndexedMzML class to make random access of indexed MzML documents possible."}, "content": {"raw": "There's no urgency, I just got around to working out the idiosyncracies of Mercurial and BitBucket so I could make a pull request.\n\nYou're correct in that `read_from_start` should probably reuse `_keepstate`. I had originally been writing this as a little one-off script and wasn't thinking about those utilities. `_keepstate` as it is right now isn't appropriate for a function which needs to reset a file-like object (or a path of a file) multiple times internally, as is the case for `find_index_list`. In the event there are multiple offsets, it needs to reset the file pointer each time, but it doesn't need to know the difference between a path and a file-like object, which is negotiated by `read_from_start`. I don't know if this will ever actually happen though, the PSI spec is a bit ambiguous.\n\nI use `re` because I couldn't reliably use `lxml` to burn through the index list since there are tons of MzML files floating around which have corrupted offsets due to an old bug in ProteoWizard, so throwing a simple regex at each line of the putative index was easier than writing another `iterparse` using function with `lxml` where error checking could be thrown in. Now that the design is more stable, I'll see if `iterparse` works reliably on both corrupted and well-formed indices, as I've got examples of both.\n\nUnfortunately, this implementation is only viable for MzML because it exploits a feature of that standard. The MzML standard explicitly includes an opt-in index of byte offsets which the file conversion software can write at the end of each MzML file. They assume we can apply this type of hacky read-from-the-end approach so that we don't have to parse the whole file in order to get to the pre-built offset index. While conceivably we could build an index for any file by trawling the bytestream with a counter and a tokenizer, this would necessarily require parsing the whole file to build, it would be highly use-case dependent whether this extra effort is worthwhile. I'll see how hard it is to do.", "markup": "markdown", "html": "<p>There's no urgency, I just got around to working out the idiosyncracies of Mercurial and BitBucket so I could make a pull request.</p>\n<p>You're correct in that <code>read_from_start</code> should probably reuse <code>_keepstate</code>. I had originally been writing this as a little one-off script and wasn't thinking about those utilities. <code>_keepstate</code> as it is right now isn't appropriate for a function which needs to reset a file-like object (or a path of a file) multiple times internally, as is the case for <code>find_index_list</code>. In the event there are multiple offsets, it needs to reset the file pointer each time, but it doesn't need to know the difference between a path and a file-like object, which is negotiated by <code>read_from_start</code>. I don't know if this will ever actually happen though, the PSI spec is a bit ambiguous.</p>\n<p>I use <code>re</code> because I couldn't reliably use <code>lxml</code> to burn through the index list since there are tons of MzML files floating around which have corrupted offsets due to an old bug in ProteoWizard, so throwing a simple regex at each line of the putative index was easier than writing another <code>iterparse</code> using function with <code>lxml</code> where error checking could be thrown in. Now that the design is more stable, I'll see if <code>iterparse</code> works reliably on both corrupted and well-formed indices, as I've got examples of both.</p>\n<p>Unfortunately, this implementation is only viable for MzML because it exploits a feature of that standard. The MzML standard explicitly includes an opt-in index of byte offsets which the file conversion software can write at the end of each MzML file. They assume we can apply this type of hacky read-from-the-end approach so that we don't have to parse the whole file in order to get to the pre-built offset index. While conceivably we could build an index for any file by trawling the bytestream with a counter and a tokenizer, this would necessarily require parsing the whole file to build, it would be highly use-case dependent whether this extra effort is worthwhile. I'll see how hard it is to do.</p>", "type": "rendered"}, "created_on": "2016-04-03T20:44:02.939729+00:00", "user": {"display_name": "Joshua Klein", "uuid": "{919f0add-304d-4b9a-8889-d2622a3dbc96}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B919f0add-304d-4b9a-8889-d2622a3dbc96%7D"}, "html": {"href": "https://bitbucket.org/%7B919f0add-304d-4b9a-8889-d2622a3dbc96%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/7d0e70bc74f783efa621a2bdd228ca22d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsJK-3.png"}}, "nickname": "mobiusklein", "type": "user", "account_id": "557058:ff82222f-afe5-4135-a1b7-8de99a00f669"}, "updated_on": "2016-04-03T20:44:03.031429+00:00", "type": "pullrequest_comment", "id": 16675692}