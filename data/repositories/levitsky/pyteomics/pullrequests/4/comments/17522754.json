{"links": {"self": {"href": "data/repositories/levitsky/pyteomics/pullrequests/4/comments/17522754.json"}, "html": {"href": "#!/levitsky/pyteomics/pull-requests/4/_/diff#comment-17522754"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 4, "links": {"self": {"href": "data/repositories/levitsky/pyteomics/pullrequests/4.json"}, "html": {"href": "#!/levitsky/pyteomics/pull-requests/4"}}, "title": "Added an IndexedMzML class to make random access of indexed MzML documents possible."}, "content": {"raw": "I've revised the XML parsing scheme to use a layered approach where the lowest layer is responsible for viewing the input binary file as blocks of data that may contain tokens that delimit XML tags, producing chunks of data exactly one XML tag at a time regardless of white space. It should address your concerns about assumptions made about the format of the input. I believe I caught the problem points between `bytes` and `str` in Python 3, ensuring that the input patterns are bytes though I'm not certain of whether I should coerce them from strings or if the end user is aware of this. I read Armin Ronacher's diatribe on the divide and I may be assuming the incompatibility is more extreme than it is. \n\nI moved a lot of the logic from the original `IndexedMzML` implementation into a generic `IndexedXML` class, and let that deal with the common problem of constructing the byte offset index and doing the look ups correctly. This made the implementation of `IndexedMzIdentML` straightforwards. Running side by side on a file I usually parse as part of my processing pipeline, I ran the following benchmark:\n\n```\nreader = mzid.MzIdentML(mzid_path, retrieve_refs=True, iterative=False)\ncached_reader = mzid.MzIdentML(mzid_path, retrieve_refs=True, iterative=False, build_id_cache=True)\nindexed_reader = mzid.IndexedMzIdentML(mzid_path, retrieve_refs=True, iterative=False)\n\n%timeit len(list(reader.iterfind(\"ProteinDetectionHypothesis\", retrieve_refs=True, recursive=False, iterative=True)))\n# 1 loops, best of 3: 3.41 s per loop\n%timeit len(list(cached_reader.iterfind(\"ProteinDetectionHypothesis\", retrieve_refs=True, recursive=False, iterative=True)))\n# 10 loops, best of 3: 164 ms per loop\n%timeit len(list(indexed_reader.iterfind(\"ProteinDetectionHypothesis\", retrieve_refs=True, recursive=False, iterative=True)))\n# 10 loops, best of 3: 168 ms per loop\n```\nThis shows that the Indexed version is just about as fast as the version that loads all of the relevant XML elements into memory for the duration of the parse. Since both the byte offset index and that element cache eat memory though, I ran a space comparison with `pympler`. \n\n```\nasizeof.asizeof(reader)\n# 14640\nasizeof.asizeof(indexed_reader)\n# 5250328\nasizeof.asizeof(cached_reader)\n# 7599712\n```\n\nSo a savings of just a little over 30% in space. Its possible to get more granular control in the time-space tradeoff by changing which element types you include in the index, which may be worthwhile if I start dealing with more complex data sets on a more restricted machine.\n\nThese implementations appear to pass all existing unit tests. ", "markup": "markdown", "html": "<p>I've revised the XML parsing scheme to use a layered approach where the lowest layer is responsible for viewing the input binary file as blocks of data that may contain tokens that delimit XML tags, producing chunks of data exactly one XML tag at a time regardless of white space. It should address your concerns about assumptions made about the format of the input. I believe I caught the problem points between <code>bytes</code> and <code>str</code> in Python 3, ensuring that the input patterns are bytes though I'm not certain of whether I should coerce them from strings or if the end user is aware of this. I read Armin Ronacher's diatribe on the divide and I may be assuming the incompatibility is more extreme than it is. </p>\n<p>I moved a lot of the logic from the original <code>IndexedMzML</code> implementation into a generic <code>IndexedXML</code> class, and let that deal with the common problem of constructing the byte offset index and doing the look ups correctly. This made the implementation of <code>IndexedMzIdentML</code> straightforwards. Running side by side on a file I usually parse as part of my processing pipeline, I ran the following benchmark:</p>\n<div class=\"codehilite\"><pre><span></span><span class=\"n\">reader</span> <span class=\"o\">=</span> <span class=\"n\">mzid</span><span class=\"p\">.</span><span class=\"n\">MzIdentML</span><span class=\"p\">(</span><span class=\"n\">mzid_path</span><span class=\"p\">,</span> <span class=\"n\">retrieve_refs</span><span class=\"o\">=</span><span class=\"n\">True</span><span class=\"p\">,</span> <span class=\"n\">iterative</span><span class=\"o\">=</span><span class=\"n\">False</span><span class=\"p\">)</span>\n<span class=\"n\">cached_reader</span> <span class=\"o\">=</span> <span class=\"n\">mzid</span><span class=\"p\">.</span><span class=\"n\">MzIdentML</span><span class=\"p\">(</span><span class=\"n\">mzid_path</span><span class=\"p\">,</span> <span class=\"n\">retrieve_refs</span><span class=\"o\">=</span><span class=\"n\">True</span><span class=\"p\">,</span> <span class=\"n\">iterative</span><span class=\"o\">=</span><span class=\"n\">False</span><span class=\"p\">,</span> <span class=\"n\">build_id_cache</span><span class=\"o\">=</span><span class=\"n\">True</span><span class=\"p\">)</span>\n<span class=\"n\">indexed_reader</span> <span class=\"o\">=</span> <span class=\"n\">mzid</span><span class=\"p\">.</span><span class=\"n\">IndexedMzIdentML</span><span class=\"p\">(</span><span class=\"n\">mzid_path</span><span class=\"p\">,</span> <span class=\"n\">retrieve_refs</span><span class=\"o\">=</span><span class=\"n\">True</span><span class=\"p\">,</span> <span class=\"n\">iterative</span><span class=\"o\">=</span><span class=\"n\">False</span><span class=\"p\">)</span>\n\n<span class=\"nf\">%timeit</span> <span class=\"n\">len</span><span class=\"p\">(</span><span class=\"n\">list</span><span class=\"p\">(</span><span class=\"n\">reader</span><span class=\"p\">.</span><span class=\"n\">iterfind</span><span class=\"p\">(</span><span class=\"s\">&quot;ProteinDetectionHypothesis&quot;</span><span class=\"p\">,</span> <span class=\"n\">retrieve_refs</span><span class=\"o\">=</span><span class=\"n\">True</span><span class=\"p\">,</span> <span class=\"n\">recursive</span><span class=\"o\">=</span><span class=\"n\">False</span><span class=\"p\">,</span> <span class=\"n\">iterative</span><span class=\"o\">=</span><span class=\"n\">True</span><span class=\"p\">)))</span>\n<span class=\"cp\"># 1 loops, best of 3: 3.41 s per loop</span>\n<span class=\"nf\">%timeit</span> <span class=\"n\">len</span><span class=\"p\">(</span><span class=\"n\">list</span><span class=\"p\">(</span><span class=\"n\">cached_reader</span><span class=\"p\">.</span><span class=\"n\">iterfind</span><span class=\"p\">(</span><span class=\"s\">&quot;ProteinDetectionHypothesis&quot;</span><span class=\"p\">,</span> <span class=\"n\">retrieve_refs</span><span class=\"o\">=</span><span class=\"n\">True</span><span class=\"p\">,</span> <span class=\"n\">recursive</span><span class=\"o\">=</span><span class=\"n\">False</span><span class=\"p\">,</span> <span class=\"n\">iterative</span><span class=\"o\">=</span><span class=\"n\">True</span><span class=\"p\">)))</span>\n<span class=\"cp\"># 10 loops, best of 3: 164 ms per loop</span>\n<span class=\"nf\">%timeit</span> <span class=\"n\">len</span><span class=\"p\">(</span><span class=\"n\">list</span><span class=\"p\">(</span><span class=\"n\">indexed_reader</span><span class=\"p\">.</span><span class=\"n\">iterfind</span><span class=\"p\">(</span><span class=\"s\">&quot;ProteinDetectionHypothesis&quot;</span><span class=\"p\">,</span> <span class=\"n\">retrieve_refs</span><span class=\"o\">=</span><span class=\"n\">True</span><span class=\"p\">,</span> <span class=\"n\">recursive</span><span class=\"o\">=</span><span class=\"n\">False</span><span class=\"p\">,</span> <span class=\"n\">iterative</span><span class=\"o\">=</span><span class=\"n\">True</span><span class=\"p\">)))</span>\n<span class=\"cp\"># 10 loops, best of 3: 168 ms per loop</span>\n</pre></div>\n\n\n<p>This shows that the Indexed version is just about as fast as the version that loads all of the relevant XML elements into memory for the duration of the parse. Since both the byte offset index and that element cache eat memory though, I ran a space comparison with <code>pympler</code>. </p>\n<div class=\"codehilite\"><pre><span></span>asizeof.asizeof(reader)\n# 14640\nasizeof.asizeof(indexed_reader)\n# 5250328\nasizeof.asizeof(cached_reader)\n# 7599712\n</pre></div>\n\n\n<p>So a savings of just a little over 30% in space. Its possible to get more granular control in the time-space tradeoff by changing which element types you include in the index, which may be worthwhile if I start dealing with more complex data sets on a more restricted machine.</p>\n<p>These implementations appear to pass all existing unit tests. </p>", "type": "rendered"}, "created_on": "2016-04-20T19:21:32.328207+00:00", "user": {"display_name": "Joshua Klein", "uuid": "{919f0add-304d-4b9a-8889-d2622a3dbc96}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B919f0add-304d-4b9a-8889-d2622a3dbc96%7D"}, "html": {"href": "https://bitbucket.org/%7B919f0add-304d-4b9a-8889-d2622a3dbc96%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/7d0e70bc74f783efa621a2bdd228ca22d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsJK-3.png"}}, "nickname": "mobiusklein", "type": "user", "account_id": "557058:ff82222f-afe5-4135-a1b7-8de99a00f669"}, "updated_on": "2016-04-20T19:21:32.565316+00:00", "type": "pullrequest_comment", "id": 17522754}